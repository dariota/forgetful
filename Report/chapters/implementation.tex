Two initial approaches to create the tool were considered: hooking directly into the compiler to detect the pattern and automatically patch it (when enabled, and when the pattern is detected with sufficiently high confidence); or creating a plug-in for an existing static analysis platform which could be manually run on existing codebases to detect the pattern. A decision was made, largely for reasons of pragmatism and convenience, to follow the second approach.

\todo{add discussion of pros and cons of both sides}

\section{Goals of the Plug-in}

\todo{What are the initial goals of the plug-in, including those that weren't achieved? How do these goals directly relate to the patch?}

There were a small set of goals for the plug-in to achieve, both functional and non-functional.

\subsection{Non-Functional Goals}

The non-functional goals are as follows:

\begin{itemize}
	\item There should be little to no modification of any existing code required to use the plug-in to a satisfactory degree
	\item There should be a minimal amount of false positives wherein the plug-in suggests a site where the patch cannot be applied
	\item Interaction with the plug-in should match the normal mode of interaction for the platform it builds on
\end{itemize}

These goals should ensure that the barrier to entry to using the plug-in is as low as possible, as it can be used directly on existing code, even if the static analysis platform itself has never been used on that code. Additionally, avoiding false positives makes it more likely that action will be taken on the plug-in's results by minimising the amount of data users have to trawl through~\cite{infervideo}. Lastly, ensuring all interaction with the plug-in matches what's expected of its platform makes its adoption in systems already using the static analysis platform even easier.

\subsection{Functional Goals}

The functional goals are as follows:

\begin{itemize}
	\item When a site where the patch can be applied is found, the user should be notified
	\item Where possible, a diff patch\footnote{A diff patch is an encoding of a set of changes that can be automatically applied with a standard tool to a file to effect a change} should be produced to apply the patch easily
\end{itemize}

Notifying the user is a fairly self-explanatory goal, as there's no point detecting an issue and not noting it. The exact form of the notification isn't important, but should provide as much information as possible without overwhelming the user, allowing them to make a reasonable decision about what action to take.

The diff patch is more complicated, but would be incredibly useful. If the plug-in's could guarantee that a certain site could be patched safely before producing a diff patch, it could be added into a pre-compilation step to rewrite the pattern silently. This would allow the source code that users work on to remain simple and as they wrote it while gaining any performance benefit from the patch.

\section{Static Analysis Platform}

There are a number of static analysis tools built for C over the years, of which a small number were chosen based on apparent activity of their development and popularity (as a proxy for likelihood to be well supported and modern). The short-list which the eventual target platform was chosen from consisted of \toolname{clang-analyzer}~\cite{clang-analyzer}, \toolname{Frama-C}~\cite{frama}, and \toolname{Infer}~\cite{fbinfer}.

\toolname{clang-analyze} is written in C++, matching the \toolname{clang} codebase in originates from and resides in. \toolname{Frama-C} and \toolname{Infer} are both written in OCaml, though while \toolname{Frama-C} builds up its own AST\footnote{An Abstract Syntax Tree (AST) is a tree-based representation of a program, with each node representing a construct appearing in the source code}, \toolname{Infer} hooks into \toolname{clang-analyzer}.

The tools that were not chosen are discussed in further depth in Chapter~\ref{state_of_the_art} in comparison to \toolname{Frama-C} in a retrospective manner.

\section{The \toolname{Frama-C} Platform}

The static analysis platform chosen was \toolname{Frama-C}. \toolname{Frama-C} has an emphasis on on correctness, providing its own language for functional specifications which can be provided alongside the code. While this is of no particular interest to this project thanks to the first functional goal, it assists in reducing false positives thanks to its conservative approach and care around sites of potential undefined behaviour~\cite{framauser}. Additionally, that specification language is used by the platform to provide properties of standard library functions such as \malloc{} and \free{}, which is essential to the project's analysis.

However, and of more interest to the project, it also has a plug-in architecture, which makes it easy to extend and build on. In particular, it enables plug-ins to interact, which allows new plug-ins to use functionality exposed by existing plug-ins thereby reducing the workload required within the plug-in itself. This was the primary factor in the choice of \toolname{Frama-C} over the other two platforms~\cite{framaarch}.

\subsection{Source Code Processing}

\toolname{Frama-C} produces an AST which plug-ins can then operate on. The version of the code exposed to plug-ins is normalised by Frama-C, which prevents duplication of efforts in handling unusual edge cases enabled by C's permissive design. As an example, consider the following C functions (which are intentionally contrived)

\lstinputlisting[style=CStyle]{samples/contrived.c}

This is normalised into something like the following by \toolname{Frama-C}

\lstinputlisting[style=CStyle]{samples/normalised.c}

We note in particular that rewrites are performed in order to avoid multiple operations occurring on a single line, such as splitting out the evaluation of return values and their actual return, or the evaluation of expressions involving function calls and the actual function call. This prevents an arbitrarily complex AST from being constructed.

The AST itself as provided to plug-ins to traverse is also annotated. It can be annotated in the source code itself, using \toolname{Frama-C}'s ACSL to add specifications, or annotations can be added by other plug-ins as they discover properties of the code~\cite{framaplug}.

The root of the AST is a representation of the file being processed, which contains a collection of globals, of which we're only interested in functions. Other globals include declarations of variables, types, structs, unions, enums, and some other miscellaneous things.

Within a function node we're interested in its statement list, which contains statements of various kinds, such as a plain instruction with no control flow, which can include a variable declaration and assignment, or a reassignment of an existing variable. These are the exact subsets of statements in which a \malloc{} can occur after normalisation of the AST, including the unusual case of a \malloc{} that's not assigned to anything.

\toolname{Frama-C} alone doesn't provide any sort of value or escape analysis, instead leaving this to be provided by plug-ins. The primary plug-in providing these features is called \toolname{Evolved Value Analysis} (\toolname{EVA}). Note that this distinction is largely symbolic, as \toolname{EVA} is statically connected to the \toolname{Frama-C} kernel, unlike regular plug-ins.

\subsection{The \toolname{Evolved Value Analysis} Plug-in}

\toolname{EVA} provides, at any given point in the AST, a set or interval describing values possible at a given point. Values can be requested for expressions or variables with respect to a given statement, and they can be evaluated either before or after execution of that statement. \toolname{EVA} also performs semantic constant folding, allowing it to be used even on code including loops~\cite{framaeva}.

Values can be described as a discrete set of values, as an interval, or as an interval skipping regular values. When \toolname{EVA} determines that one of the representations is becoming too large, it can degenerate the value to a broader description that contains all of the original values. As an example, take the following:

\lstinputlisting[style=CStyle,firstline=21,lastline=38]{samples/eva.c}

Assuming that the level of semantic constant folding \toolname{Frama-C} is permitted to do is high enough to fully evaluate the loop and that \texttt{NUM\_PRIMES} is set to 8, \toolname{EVA} produces the following values at the end of the loop

\begin{table}
	\centering
	\begin{tabular}{lc|lc}
		\toprule
		\textbf{Variable} & \textbf{Values} & \textbf{Variable} & \textbf{Values} \\
		\midrule
		\texttt{morePrimes[0]} & \{2\} & \texttt{morePrimes[4]} & \{11\} \\
		\texttt{morePrimes[1]} & \{3\} & \texttt{morePrimes[5]} & \{13\} \\
		\texttt{morePrimes[2]} & \{5\} & \texttt{morePrimes[6]} & \{17\} \\
		\texttt{morePrimes[3]} & \{7\} & \texttt{morePrimes[7]} & \{19\} \\
		\texttt{randVal} & [0..32767] & \texttt{index} & \{8\} \\
		\texttt{randPrime} & \{2; 3; 5; 7; 11; 13; 17; 19\} & & \\
		\bottomrule
	\end{tabular}
\end{table}

In particular, note that each item in the array \texttt{morePrimes} is tracked separately by \toolname{EVA}, that \texttt{randPrime} can take on any of the prime values, that \texttt{randVal} can take on any values between 0 and \toolname{Frama-C}'s \texttt{RAND\_MAX}, and that variables that can only take on a single value are considered to have a value which is a singleton set.

Next, we consider the values reported if the semantic constant folding allowed is set too low to evaluate anything past the first prime. The values reported are now

\begin{table}[h]
	\centering
	\begin{tabularx}{\linewidth}{>{\hsize=1.1\hsize}X >{\hsize=1.3\hsize}X | >{\hsize=0.6\hsize}X >{\hsize=1\hsize}X}
		\toprule
		\textbf{Variable} & \textbf{Values} & \textbf{Variable} & \textbf{Values} \\
		\midrule
		\texttt{morePrimes[0]} & \{2\} & \texttt{index} & \{8\} \\
		\texttt{morePrimes[1..7]} & [3..2147483647] or UNINITIALIZED & \texttt{randVal} & [0..32767] \\
		\texttt{randPrime} & [2..2147483647] & & \\
		\bottomrule
	\end{tabularx}
\end{table}

This time we note that EVA can no longer determine whether the loop ever terminates and cannot determine the values for all indices of the \texttt{morePrimes} array, nor if they're ever initialised (due to the possibility of overflow in \texttt{current} without index being incremented sufficient times to exit the loop). As expected, \texttt{randPrime}'s possible values also cannot be determined, as it depends on full evaluation of all values in \texttt{morePrimes}. However, the other variables do not depend on the loop, so they can be correctly evaluated regardless.

Next, increasing \texttt{NUM\_PRIMES} to 9 causes \toolname{EVA} to decide \texttt{randPrime} has too many values, so it reduces its precision to [2..23] which still contains all the correct values with as much precision as possible without storing the individual values.

Additionally, changing line 12 so that \texttt{randPrime} is assigned \texttt{current * 4} causes \toolname{EVA} to again degenerate its set of values into [2..76]0\%2, which is the most complicated value type \toolname{EVA} can produce, and indicates that values start at 2 with an offset of 0 and every second value is potentially valid. This includes all the valid values (\{8; 12; 20; 28; 44; 52; 68; 76\}), but is less precise than an alternative interval of [8..76]0\%4. It's not clear why \toolname{EVA} choose one instead of the other.

While this covers all simple values such as integers, floats, and even structs (which function similar to an array, where each member is separately displayed), it doesn't cover pointers. There are two kinds of pointer which are represented identically. The first is a pointer to an existing variable such as \texttt{\&randPrime}, while the second is a pointer created by a call to a function like \malloc{}. Both types are represented as \texttt{\{\{ \&varname \}\}}, where \texttt{varname} is either the name of the variable pointed to, or something of the form \texttt{\{\{\&\_\_malloc\_main\_l33 \}\}} in the case of \malloc{}, where a unique variable name is generated, representing a point in heap memory which \toolname{EVA} calls a Base.

Bases can be collected in sets, same as regular values, but cannot form an interval. There is also a special pointer, \texttt{NULL}, representing exactly that and marked as a potential return value by \toolname{Frama-C}'s internal version of \malloc{}, although that can be disabled by one of \toolname{EVA}'s options~\cite{framamalloc}. Clearly, bases are of particular interest for the project.

\section{Development of the \toolname{Forgetful} Plug-in}

\subsection{Visitor Pattern}

\todo{Describe how the visitor pattern is used}

\subsection{Allocation Tracking}

\todo{describe the process used to track the allocation throughout a program}

\subsection{Difficulties Encountered}

What difficulties were encountered (new language, installation, documentation, usefulness of results from other plug-ins [no location data in Base], any future issues)

\section{Final State of the Plug-in}

What state is the plug-in currently in? How well has it achieved its goals? Refer to future work section.

\subsection{Identified Sites}

\subsection{Limitations}
