\section{Results}

There are two main categories of results to be discussed, the specialised benchmarks and the real world study.

\subsection{Specialised Benchmarks}

The specialised benchmarks, while constructed to be an ideal case for application of the patch, clearly validate that the patch can result in a significant performance increase in specific cases. 

That being said, they also serve as a reminder that even in simple code there may be hidden issues and complexity, such as \functionname{alloca}'s performance at the \texttt{O0} optimisation level in both of the benchmarks, or the unexpected slowdown of the \functionname{dynamic} allocation method at the \texttt{O3} optimisation level in the sort benchmark.

The benchmarks also highlight that, in certain cases, the patch makes no performance difference (little to no difference by 16 items in the sort benchmark, and minimal at the changeover point of 64 items in the parallel benchmark).

Both the \functionname{dynamic} and \functionname{stack} allocation methods as implemented have downsides that are present even when their benefits are no longer being reaped. Note that the following all assume correct usage, for example ensuring that no pointer to the allocated buffer escapes the function scope, or is made accessible to another thread (as threads do not share stacks~\cite{threadstack}).

\subsubsection{\functionname{dynamic} Method Downsides}

The stack frame is bloated with the buffer even when it's not being used, as space must always be allocated for it even if it's determined at run-time that the buffer can't be used.

Depending on where the buffer is declared, whether multiple buffers are declared (if there are multiple inputs that can be copied to stack allocated buffers), and how large the buffers are, locality of the stack could potentially be affected.

The larger stack frames required also increase the likelihood of stack overflow occurring, which will generally result in a segmentation fault due to the invalid access to storage (outside the bounds of the stack)~\cite{c11std}.

\subsubsection{\functionname{stack} Method Downsides}

The most immediate issue with the \functionname{stack} method as implemented is that it unconditionally uses \functionname{alloca} to allocate the buffer. As mentioned before, \functionname{alloca} is not in the C standard and as such is both machine and compiler dependent.

The unconditional allocation also means that the risk of stack overflow, and hence a segmentation fault, is even greater than in the \functionname{dynamic} case, due to use of \functionname{alloca} even if the allocation to be performed is very large.

Another issue with \functionname{alloca} is that it has no way to indicate if the stack frame cannot be extended. Unlike \malloc{}, it never returns \varname{NULL}. This means that is no way to tell if an allocation was successful until the buffer is used, causing a stack overflow and segmentation fault.

\subsubsection{Handling The Downsides}

A better solution than the \functionname{dynamic} or \functionname{stack} allocation methods may be somewhere in between the two. Assuming that the lack of portability of \functionname{alloca} is not an issue, some of its benefits can be used to eliminate some downsides of the \functionname{dynamic} method. This is referred to as the \functionname{hybrid} allocation method hereafter.

Rather than performing a check on the input size and using a statically declared buffer on the stack, the stack allocation itself can be performed with \functionname{alloca} (or, under the C99 standard, a variable length array could be used with portability benefits). This would prevent bloating the stack unnecessarily, allowing only the buffers that will actually get used to be stack allocated.

To handle the increased risk of a segmentation fault, a signal handler could be written capturing the \texttt{SIGSEGV} signal raised. However, signal handlers are extremely restricted in what they can do, which functions they can call, and even which variables they can write to~\cite{signalhandling}. The increase is complexity resulting from needing to write and maintain this signal handler, if a signal handler can even be written to sufficiently generically resolve stack overflow errors\footnote{A trivial, but not necessarily useful, handler simply expands the stack when a stack overflow is detected, but this is unlikely to be possible to do for any extended amount of time, and only works in an environment can be extended at runtime using only functions permissible in signal handlers. This also requires the handler to be able to distinguish between a signal raised due to a stack overflow or due to another reason, such as a \varname{NULL} dereference.}, is unlikely to be justified by any performance benefits gained.

\subsection{Real World Benchmark}

The real world benchmark had disappointing results when compared to the specialised benchmarks, and even to Stenberg's original claims. Results were found to be indistinguishable from variance in the test environment itself, in sharp contrast with the 30\% performance increase claimed~\cite{curlmalloc}.

Despite these results, this doesn't mean that the patch is not worth applying, or at least researching further. Compiler optimisations are known to produce minimal performance benefits individually, instead building up mutually to more significant gains over time and in conjunction with each other.\\
This observation was noted in Proebsting's~Law, in which Proebsting claimed the following, with the figure of doubling every 18 years chosen to match the more well-known Moore's~Law.

\begin{quote}
	These assumptions lead to the conclusion that compiler optimization advances double computing power every 18 years. […] compiler optimizations contribute only 4\% [per year]. Basically, compiler optimization work makes only marginal contributions.~\cite{proebstingdecl}
\end{quote}

These findings were formalised 3 years later by Scott, in which the rate of improvement was estimated to be closer to 2.8–3.6\% per year, or even as high as 4.9\%, depending on what year was taken as the starting point in which compiler optimisations begun to be developed~\cite{proebstingformal}. Scott also notes that there's no reason to give up on compiler optimisations, as any performance benefit is better than none, and in particular there's no reason to turn down existing performance benefits.

\section{State of the Plug-in}

The current state of the \toolname{Forgetful} plug-in is described in Section~\ref{pluginstate}, where the instances of the pattern it can detect are shown and some of its limitations discussed. Some of these limitations are outside of the control of the plug-in, such as the inability to process functions which use recursion, which is imposed by \toolname{EVA}. Other limitations, such as only performing intra-procedural analysis, or the simplified handling of intervals, are internal limitations.

\subsection{Interval Handling}

As mentioned in Section~\ref{alloctrack}, the handling of allocations whose size is some value from an interval is simplistic, with the allocation only being tracked if the entire interval is smaller than the configured maximum allocation size.

As an alternative, intervals could be accepted if some configurable portion of the interval was smaller than the maximum allocation size. This information could then be used in conjunction with profiling data to determine whether the introduction of the \functionname{hybrid} allocation method or similar would be worthwhile.

\subsection{Intra-Procedural Analysis}

\toolname{Forgetful} is currently limited to intra-procedural analysis, due to the additional complexity that inter-procedural analysis would add and the limited time available to complete the project. Tracking the progress of allocated blocks of memory across function boundaries could significantly improve the quality and number of sites that \toolname{Forgetful} can report.

Consider the following code listing:

\lstinputlisting[style=CStyle]{samples/xmalloc.c}

Currently, \toolname{Forgetful} cannot detect that \varname{unreported} is a candidate for replacement, despite \functionname{xmalloc} being a simple wrapper on \malloc{} which cannot return \varname{NULL}. By tracking allocations across function calls, this case could be detected.

This pattern is not unusual, and is similar in structure to a constructor function which allocates and initialises a struct, again being quite a thin wrapper on \malloc{} with some setup steps on the returned value.

Barrett and Zorn defined a term for function invocations which eventually led to a \malloc{}, calling them length-N call chains, where a length-1 call chain is a direct invocation of \malloc{}. While researching the ability to predict lifetimes at allocation sites, they discovered that length-4 call chains were almost always sufficiently long to determine the lifetime of a given allocation~\cite{predictors}. \\
Their methods and goals were different to this project's, and their figures were found experimentally with a limited number of programs 25 years ago, but their findings suggest that analysing longer call-chains would be of worth.

\subsection{Notification Output}

The output displayed in Section~\ref{alloctrack} is real, but is not representative of \toolname{Forgetful}'s output. As analysis is performed on the normalised AST, it may occasionally reference statements that do not exist in the source code itself. Consider the following code listing:

\lstinputlisting[style=CStyle]{samples/transformedOut.c}

The statement on line 2 is rewritten to the following:

\lstinputlisting[style=CStyle]{samples/normalisedOut.c}

This means that when the allocation is reported, it reports the contents of line 4 as the allocation site, which makes it difficult for the user to discern what allocation is being referred to, as can be seen in the following output from the plug-in.

\lstinputlisting[breaklines=true]{samples/transformed_output.txt}

The formatting of this output raises another issue that could be revisited, which is that the output is not easily machine parsable. This could make it less useful in automated pipelines using the tool to detect issues in pull requests, for example.

\section{Benefit of Further Work}

Possibilities for future work in this space are discussed in Chapter~\ref{future} in detail, but it is worth discussing Proebsting and Scott's outlooks on further research into compiler optimisations.

\todo{notes from Scott on slight bias against optimisations due to optimisations that can't be disabled, backed up by Pugh}

\todo{notes from Scott on disproportionate improvement of FP algos}

\todo{notes from Proebsting's site about higher level things, comments from Pugh suggesting the same}

\todo{issues with existing benchmarks – they're unrealistic, low level, hand optimised, horrible style, focus on expression/statement level optimisations rather than big picture}

\todo{higher level constructs, safety checks, clean \& simple code (loop unrolling, cache aligning), ADTs as efficient as primitives, guarantees about performance (e.g. TCO in Haskell/Kotlin), swapping out algorithms for efficient versions with the same result (sorts), parallelism}

\todo{what about the benefit of an RTS and its optimisations?}
