<html>
	<head>
		<meta charset="utf-8"/>
		<title>ntun.es - forgetful project - Literature Review/State of the Art</title>
		<link rel="stylesheet" type="text/css" href="/styles/default.css">
	</head>
	<body>
		<h1 id="literature-reviewstate-of-the-art">Literature Review/State of the Art</h1>
		<p>There are a number of aspects to consider when determining what the appropriate literature/state of the art is.</p>
		<p>For</p>
		<ul>
		<li>Comparison of the plugin and the platform it built on: other static analysis tools</li>
		<li>Alternative methods to approach the problem: profiling and runtime tools</li>
		<li>Allocator methods and their performance: papers comparing them</li>
		<li>Validation of the project space: youngest generation GC, alternative allocation methods and allocators to reduce allocation burden</li>
		<li>Discussion of results: typical performance increase from compiler optimisations</li>
		</ul>
		<h2 id="plugin-and-its-platform">Plugin and its Platform</h2>
		<p>Three options are examined - <em>Frama-C</em> itself, <em>Infer</em>, and <em>clang-analyzer</em>.</p>
		<h3 id="frama-c">Frama-C</h3>
		<p>While <a href="http://frama-c.com/index.html"><em>Frama-C</em></a> was a decent choice for its extensibility and availability of introductory <a href="http://frama-c.com/plugins.html">guides to plugin development</a>, finding assistance/documentation for it was relatively difficult, even for basic functionality of <a href="http://frama-c.com/value.html">EVA</a>.<br />
		Perhaps a better choice would have been to modify/extend a static analysis tool with a more active and modern community (irregular releases consist of <a href="https://github.com/Frama-C/Frama-C-snapshot/commits/master">single dumps of source code</a>, with the first beta release in <a href="http://frama-c.com/download_hydrogen.html">March 2008</a>), which likely would've made it easier to dig into the codebase even if it wasn't built as intentionally for extensibility.<br />
		<em>Frama-C</em>'s (open source) community is not hugely active, with only a few <a href="https://bts.frama-c.com/dokuwiki/doku.php?id=mantis:frama-c:external_plugins">external plugins</a> available, and not many plugins having been created between its initial release a decade ago and now.</p>
		<h3 id="infer">Infer</h3>
		<p>To the end of finding a large and active community, Facebook's <a href="http://fbinfer.com/"><em>Infer</em></a> might have been a better choice. By its own description, &quot;Infer checks for null pointer dereferences, memory leaks, coding conventions and unavailable API’s&quot;, it may also have been well suited to the problem thanks to its emphasis on memory issues and tracking memory.<br />
		Facebook first open-sourced <em>Infer</em> in <a href="https://github.com/facebook/infer/commit/b8982270f2423864c236ff8dcdbeb5cd82aa6002">June of 2015</a>, at which point it already supported C, Objective-C, C++, Java (≤ 1.7) and contained ~100k LoC. Since then, it has averaged 4-6 commits per day (dependending on whether you count weekends), now has ~300k LoC, and supports Java 8 as well as the original languages.<br />
		While it wasn't explicitly designed for modularity like <em>Frama-C</em>, its <a href="https://github.com/facebook/infer/tree/master/infer/src">internal structure for checkers</a> is consistent and logical, which should make the addition of new checkers not prohibitively difficult.</p>
		<h3 id="clang-analyzer">clang-analyzer</h3>
		<p><a href="http://clang-analyzer.llvm.org/"><em>clang-analyzer</em></a>, which <em>Infer</em> is built on, is the only analyser in this list not written in OCaml, instead being in C++ to match the rest of the clang codebase. Among <a href="http://clang-analyzer.llvm.org/available_checks.html">features listed in its documentation</a> are a few memory analysis checkers (null, double free, stack reference escape detection) that could prove to be a useful base to build off. My familiarity with C++ (at least compared to my lack of familiarity with OCaml) could also have proven useful in development, reducing delays.<br />
		<em>clang-analyzer</em>'s development is much less active than <em>Infer</em>'s, but still more active (or at least more frequently updated) than <em>Frama-C</em>'s. It also has mailing lists and other community communication channels which could have been useful. It's the oldest of the three, <a href="https://github.com/llvm-mirror/clang/commit/e4e633400b1993c1174b47b774fa015220fa695c">started in September 2007</a>, but has had a huge amount of development since then (a caveat here: its full development lifecycle is public, from inception to its current state, unlike the other two, which can make it seem more active).<br />
		One unique mark against <em>clang-analyser</em> is its heavy emphasis on OS X related features and tutorials/setup instructions revolving around assumptions of the use of Apple hardware and software, which could cause minor delays in development.</p>
		<h2 id="alternative-approaches">Alternative Approaches</h2>
		<p>Other than static analysis/compile-time checking, run-time analysis can also be used to surface issues in the code. This profiling has an added benefit that (used properly), it can better reflect performance under real workloads. There are three main ways of achieving this, which are all different levels of the same thing.</p>
		<h3 id="custom-built-wrapper-functions">Custom Built Wrapper Functions</h3>
		<p>The first method involves a custom allocator wrapper function, which is the approach taken by <code>cURL</code>. In this method, calls to memory management functions are intercepted by redefining them to instead use the custom allocators, using something like <code>#define malloc(size) curl_domalloc(size, __LINE__, __FILE__)</code> to intercept calls and add in debugging information.<br />
		Usage of this system can usually be enabled or disabled at compile time by defining or undefining certain symbols, and a detailed implementation can be seen <a href="https://github.com/curl/curl/blob/67bd4ab19e2903b45b66278956801a681fc35520/lib/memdebug.h">in <code>cURL</code> itself</a>.<br />
		This method has the obvious downside that the system must be maintained by its users, who usually have goals orthogonal to it. However, it also allows the most fine-grained control of the three methods, and can easily be extended to also track other functions.</p>
		<h3 id="provided-allocator-wrapper-functions">Provided Allocator Wrapper Functions</h3>
		<p>When the fine-grained control of intercepting any given function, or adding more things to profile is not needed, wrapper functions can be found already written available online to be compiled along with your existing code.<br />
		Similar to the above, these intercept calls to memory management functions, outputting profiling data to a specified log file. One such example is <a href="https://panthema.net/2013/malloc_count/"><em>malloc_count</em></a>, which can also track stack usage. Since these projects are more focused on this specific task, they also attempt to meet standards, which allows the user to use their profiling data with existing graphing tools for memory profiling dumps.<br />
		While this method doesn't have the advantage of fine-tuned control of what exact data is dumped nor which functions are intercepted, it also doesn't suffer the disadvantages of maintenance and development of the system. It also benefits over the next method by being faster.</p>
		<h3 id="run-time-profilinginterception">Run-time Profiling/Interception</h3>
		<p>Without requiring function interception to be included in code at compile time, hooks can be used to intercept calls to any given function. Programs such as <a href="http://valgrind.org/"><em>Valgrind</em></a> can use this functionality, for example, to track memory leaks at run-time by tracking all allocated and freed memory, or determine when uninitialised data is used as if it was initialised (as a branch condition, for example).<br />
		Similarly, programs such as <a href="https://github.com/KDE/heaptrack"><em>Heaptrack</em></a> use <a href="https://en.wikipedia.org/wiki/Hooking">hooks</a> in order to profile memory usage. As such, it requires no changes to the actual source or files included for compilation. However, in order to get more detailed information about allocations (such as the line in the source where it occurs), debug symbols must be included in the binary, which involves a minor change to the compilation procedure.<br />
		As has been the trend in this section, the increased convenience comes with a decrease in tunability. However, again in comes with an improvement in ease of consumption of data - <em>Heaptrack</em> is a two part tool, one producing a detailed dump while the other consumes the dump to produce a large number of statistics and graphics.</p>
		<h2 id="allocator-methods-and-their-performance">Allocator Methods and their Performance</h2>
		<p>There are knock-on effects on performance to consider as a side effect of the optimisation, and direct effects that can moderate the effectiveness of the optimisation.</p>
		<h3 id="locality">Locality</h3>
		<p>One of the knock-on effects is on locality of the data. Replacing dynamic allocations with stack allocation could affect cache-hit frequency, as the stack is likely to be in cache, whereas the next section of the heap may not be. However, in <em>An Empirical and Analytic Study of Stack vs. Heap Cost for Languages with Closures, Appel, Shao, 1996</em>, they found the effect of using a stack or a heap for frame allocation to be too trivial to matter in terms of effect on the cache miss rate.<br />
		They do note that the cache write-miss rate is very high for heap-allocated frames, but this can be mitigated with an appropriate write-miss strategy.<br />
		In short, it seems unlikely that the cache miss/hit rate will have a significant impact on the results of the optimisation, but of course architectures and timings have changed over the past two decades - what was trivial then may not be any more (as cache increases in relative speed).</p>
		<h3 id="real-time-considerations">Real-Time Considerations</h3>
		<p>Another consideration for the utility of the optimisation is the different potential target users. For systems with real-time considerations, using stack allocation instead of heap allocation can be beneficial even without improvement in average case performance increase, due to capping the worst-case performance.<br />
		In <em>Real-Time Performance of Dynamic Memory Allocation Algorithms, Puaut, 2002</em>, she finds that ratio of average to worst-case performance (obtained analytically) of memory allocation algorithms varies from about 10-10000 (the lower bound algorithms have an average case of about 10x those of the higher bound), while stack allocation has fixed performance, even if bounds checking is added. Of course, similar benefits could be obtained with correct use of an allocator designed for that purpose.<br />
		However, Puaut also finds that the actual observed ratio of average to worst-case performance ranges from about 1-35, so for real workloads the effect may, again, not be great.</p>
		<h2 id="validation-of-project-space">Validation of Project Space</h2>
		<p>In order to validate the project space, other attempts to target the same inefficiencies were searched for. Small amounts of short-lived memory is the purview of the youngest generation of generational garbage collection, while other approaches to tackling the same issue could instead go directly to the allocator method in order to explicitly split out these allocations ahead of time.</p>
		<h3 id="generational-garbage-collection">Generational Garbage Collection</h3>
		<p>Generational (or ephemeral) garbage collection relies on a hypothesis, supported by empirical measurements, that the most recently created objects are also those most likely to become unreachable quickly.<br />
		Supporting this hypothesis, in <em>Uniprocessor Garbage Collection Techniques, Wilson, 1992</em>, he claims that, while figures vary depending on the source language and program, 80-98% of all newly-allocated objects &quot;die within a few million instructions, or before another megabyte has been allocated; the majority of objects die even more quickly, within tens of kilobytes of allocation&quot;.</p>
		<p>In terms of efficiency, <em>Garbage Collection Can Be Faster Than Stack Allocation, Appel, 1987</em> makes the counter-intuitive claim the title suggests it will. The claim relies on the usage of a copying garbage collector and a sufficiently large amount of physical memory being made available. Key to the assertion is that not every allocation will need to be handled once it's unreachable, as only objects that survive until the next garbage collection need to be handled by copying them to the new heap.<br />
		Exact formulae are provided in the paper, parametrisable in terms of: the memory available; instructions to copy an object; average size of an object; instructions to explicitly free an item; number of allocated items; instructions to traverse the object graph. However, the conclusion is that even arbitrarily efficient explicit freeing always eventually loses to larger amounts of available memory.</p>
		<h3 id="direct-to-allocator">Direct to Allocator</h3>
		<p>There are cases where garbage collection can't be used for one reason or another (insufficient timing guarantees for real-time systems for example), so in these cases alternate systems can be used to take advantage of the large proportion of short-lived allocations.</p>
		<p><em>Using Lifetime Predictors to Improve Memory Allocation Performance, Barrett, Zorn, 1993</em> discusses the use of profiling to determine which allocations are short-lived. In particular, they describe an algorithm for lifetime prediction using a combination of profiling data, allocation site and allocation size which (depending on which program they used to benchmark) correctly predicted the lifetimes of 42-99% of allocations. Clearly these results vary too widely (and only 4 programs were benchmarked) to be a definitive indicator of whether this method is worthwhile.<br />
		However, in the same paper, simulated results using lifetime prediction to segregate allocations into specialised areas in the heap (with shorter-lived allocations/deallocations being cheaper) showed that there was significant potential for reduction of memory overhead, improvement of reference locality (by having recent allocations in a small section of the heap likely to be in cache) and occasionally improvement of performance.</p>
		<h2 id="results">Results</h2>
		<p>There are two sets of results to discuss (one for each of the case studies). While the real world case study was less than impressive, there are a few points to moderate it.</p>
		<h3 id="specialised-benchmarks">Specialised Benchmarks</h3>
		<p>The specialised benchmarks, being constructed to be an ideal case, clearly validate that the optimisation can be worthwhile in specific cases.</p>
		<p>That being said, they also serve as a reminder that even in simple code there may be hidden issues and complexity, in particular <code>alloca</code>'s performance under <code>O0</code> in both the parallel and sort benchmarks and the unexpected drop in the dynamic method's performance under <code>O3</code> in the sort benchmark.</p>
		<p>Lastly, it also highlights that in certain cases the optimisation makes no performance difference (a wash by 16 items in the sort benchmark, minimal by 64 items in the parallel benchmark) in certain cases while still having the negative effects (very large stack, increased risk of stack overflow, programmer error leading to escaping pointers to stack allocated items).</p>
		<h3 id="curl-benchmarks">cURL Benchmarks</h3>
		<p>With Stenberg's claims about performance increases and removal of huge amounts of <code>malloc</code>s, the results found are quite disappointing, being indistinguishable from variance in the test environment itself.</p>
		<p>However, this doesn't mean the optimisation isn't worth performing. Compiler optimisations produce minimal performance benefits over time, and build up mutually to more significant gains over time and in conjunction with each other.</p>
		<p>This was somewhat formalised by <a href="http://proebsting.cs.arizona.edu/law.html">Proebsting's Law</a> (Todd A. Proebsting of University of Arizona), which claimed &quot;compiler optimization advances double computing power every 18 years&quot;, with figures chosen to reasonably match up with the better known Moore's Law. This is equivalent to 4% increases per year, which makes even vanishingly small improvements in any given case seem better by comparison.</p>
		<p>Proebsting's law is further researched in <em>On Proebsting’s law, Scott, 2001</em>, in which Scott finds it to most likely be true, producing a range of possible figures for yearly improvement between 2.8-4.9% depending on a few factors.<br />
		However, this doesn't mean optimisations aren't worthwhile. He also refers to a lecture by Bill Pugh (University of Maryland) titled <em>Is Code Optimization (Research) Relevant</em>, created in response to Proebsting's Law, in which Pugh argues no one will turn down a free performance improvement from compiler optimisations, but suggests that focus on producing optimisations for high-level constructs to free up programmers to be more productive could be more worthwhile.</p>
	</body>
</html>
